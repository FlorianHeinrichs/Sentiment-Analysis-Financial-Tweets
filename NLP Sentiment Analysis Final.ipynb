{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make imports and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import spacy\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from joblib import dump\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from spacy.util import minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('data/all-data-v2.csv', sep=';', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = raw_data[raw_data['sentiment']=='positive'].sample(175)\n",
    "data = data.append(raw_data[raw_data['sentiment']=='negative'].sample(35), ignore_index=True)\n",
    "data = data.append(raw_data[raw_data['sentiment']=='neutral'].sample(70), ignore_index=True)\n",
    "\n",
    "def remove_quot_mark(string):\n",
    "    if string[0] == ' ':\n",
    "        string = string[1:]\n",
    "    if string[0] == '\"':\n",
    "        string = string[1:]\n",
    "    if string[-1] =='\"':\n",
    "        string = string[:-1]\n",
    "    return string\n",
    "        \n",
    "data['sentence'] = data['sentence'].apply(remove_quot_mark)\n",
    "data.index = np.arange(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive sentences: 175\n",
      "Number of negative sentences: 35\n",
      "Number of neutral sentences: 70\n"
     ]
    }
   ],
   "source": [
    "val_counts = data['sentiment'].value_counts()\n",
    "\n",
    "print('Number of positive sentences: ' + str(val_counts['positive']) + '\\n' \n",
    "      + 'Number of negative sentences: ' + str(val_counts['negative']) + '\\n' \n",
    "      + 'Number of neutral sentences: ' + str(val_counts['neutral']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data is unbalanced! Give more weight to negative and neutral sentences (e.g. through resampling)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create column with labels (1: positive, 0: neutral, -1: negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "      <td>The long-standing partnership and commitment e...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>During the past decade it has gradually divest...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>The company also estimates the already carried...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>In the second quarter of 2010, the group 's pr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>The OMX Nordic 40 ( OMXN40 ) index, comprising...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                           sentence  label\n",
       "0  positive  The long-standing partnership and commitment e...      1\n",
       "1  positive  During the past decade it has gradually divest...      1\n",
       "2  positive  The company also estimates the already carried...      1\n",
       "3  positive  In the second quarter of 2010, the group 's pr...      1\n",
       "4  positive  The OMX Nordic 40 ( OMXN40 ) index, comprising...      1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['label'] = data.apply(lambda row: int(row['sentiment']=='positive') - int(row['sentiment']=='negative'), axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train = data['sentence']\n",
    "y_train = data['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define (auxiliary) functions for models based on \"Bag of Words\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**label_to_dict(label):**\n",
    "* The function *label_to_dict* takes as input labels in the set $\\{-1, 0, 1\\}$ and returns them in a nested dictionary with outer key 'cats' (categories).\n",
    "\n",
    "* Example:\n",
    "    * Input: 1\n",
    "    * Output: {'cats': {'1': True, '-1': False, '0': False}}\n",
    "    \n",
    "**transform_bow(X_train, y_train):**\n",
    "* The function *transform_bow* generates data through resampling from the underrepresented classes (negative and neutral) and returns a list of tupels (X_train, y_train).\n",
    "\n",
    "**train_bow(X_train, y_train, architecture):**\n",
    "* The function *train_bow* trains a model based on the \"Bag of Words\" representation of the training data (X_train, y_train) and returns the trained model.\n",
    "* The parameter architecture can be chosen from the set $\\{$'bow', 'simple_cnn', 'ensemble'$\\}$.\n",
    "\n",
    "**validate_bow(X_train, X_test, y_train, y_test, architecture):** \n",
    "* The function *validate_bow* trains the model train_bow(X_train, y_train, architecture) and returns a confusion matrix based on the test data (X_test, y_test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_dict(label):\n",
    "    return {'cats': {'1': label == 1,\n",
    "                    '-1': label == -1,\n",
    "                    '0': label == 0}}\n",
    "\n",
    "def transform_bow(X_train, y_train):\n",
    "    \n",
    "    X_train_2d = pd.concat([X_train,pd.Series(np.zeros(len(X_train)),\n",
    "                                              index=X_train.index)], axis=1)\n",
    "    ros = RandomOverSampler()\n",
    "    X_train, y_train = ros.fit_resample(X_train_2d, y_train)\n",
    "    X_train = X_train['sentence']\n",
    "        \n",
    "    y_train_dict = y_train.apply(lambda label: label_to_dict(label))\n",
    "    train_data = list(zip(X_train, y_train_dict))\n",
    "    return train_data\n",
    "\n",
    "def train_bow(X_train, y_train, architecture):\n",
    "    model = spacy.blank('en')\n",
    "        \n",
    "    train_data = transform_bow(X_train, y_train)\n",
    "\n",
    "    textcat = model.create_pipe(\"textcat\", config={\"exclusive_classes\": True, \n",
    "                                                   \"architecture\": architecture})\n",
    "    model.add_pipe(textcat)\n",
    "    textcat.add_label('1')\n",
    "    textcat.add_label('-1')\n",
    "    textcat.add_label('0')\n",
    "    optimizer = model.begin_training()\n",
    "    losses = {}\n",
    "    \n",
    "    for epoch in range(10):\n",
    "        random.shuffle(train_data)\n",
    "        batches = minibatch(train_data, size=8)\n",
    "\n",
    "        for batch in batches:\n",
    "            texts, labels = zip(*batch)\n",
    "            model.update(texts, labels, sgd = optimizer, losses=losses)\n",
    "    \n",
    "    return model\n",
    "    \n",
    "def validate_bow(X_train, X_test, y_train, y_test, architecture):\n",
    "    \n",
    "    model = train_bow(X_train, y_train, architecture)\n",
    "    test_docs = [model.tokenizer(text) for text in X_test]\n",
    "    textcat = model.get_pipe('textcat')\n",
    "    scores, _ = textcat.predict(test_docs)\n",
    "    predicted_labels = [textcat.labels[label] for label in scores.argmax(axis=1)]\n",
    "    \n",
    "    return confusion_matrix(y_test.apply(str),predicted_labels, labels=['1','0','-1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define (auxiliary) functions for models based on TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**transform_tfidf_train(X_train):** \n",
    "* The function *transform_tfidf_train* generates the tf-idf representation of the training data X_train and returns this representation and the corresponding TfidfVectorizer.\n",
    "\n",
    "**transform_tfidf_test(X_test, vectorizer):** \n",
    "* The function *transform_tfidf_test* returns the tf-idf representation of the test data X_test based on the vectorizer.\n",
    "\n",
    "**train_tfidf(model_type, X_train, y_train, param):** \n",
    "* The function *train_tfidf* trains a model based on the tf-idf representation of the training data (X_train, y_train) and returns the trained model.\n",
    "* The parameter *model_type* can be chosen from the set $\\{$'RandFor-tfidf', 'Boost-tfidf', 'LogReg'$\\}$. \n",
    "* The parameter *param* represents the number of estimators ('RandFor-tfidf' and 'Boost-tfidf') or the inverse of the regularization parameter C ('LogReg').\n",
    "\n",
    "**validate_tfidf(model_type, X_train, X_test, y_train, y_test, param):** \n",
    "* The function *validate_tfidf* trains the model train_tfidf(model_type, X_train, y_train, param) and returns a confusion matrix based on the test data (X_test, y_test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_tfidf_train(X_train):\n",
    "    vectorizer = TfidfVectorizer(stop_words=stopwords.words('english'))\n",
    "    X_train = vectorizer.fit_transform(X_train).toarray()\n",
    "    return X_train, vectorizer\n",
    "\n",
    "def transform_tfidf_test(X_test, vectorizer):\n",
    "    X_test = vectorizer.transform(X_test)\n",
    "    return X_test\n",
    "\n",
    "def train_tfidf(model_type, X_train, y_train, param):\n",
    "    if model_type == 'RandFor-tfidf':\n",
    "        model = RandomForestClassifier(n_estimators = param, class_weight = 'balanced')\n",
    "    elif model_type == 'LogReg':\n",
    "        model = LogisticRegression(C = param, class_weight = 'balanced')\n",
    "    elif model_type == 'Boost-tfidf':\n",
    "        base_est = DecisionTreeClassifier(max_depth=3)\n",
    "        model = AdaBoostClassifier(base_estimator=base_est, n_estimators=param)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "def validate_tfidf(model_type, X_train, X_test, y_train, y_test, param):\n",
    "    \n",
    "    X_train, vectorizer = transform_tfidf_train(X_train)\n",
    "    X_test = transform_tfidf_test(X_test, vectorizer)\n",
    "    model = train_tfidf(model_type, X_train, y_train, param)\n",
    "    \n",
    "    predicted_labels = model.predict(X_test)\n",
    "    return confusion_matrix(y_test,predicted_labels, labels=[1,0,-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define (auxiliary) functions for models based on Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**transform_w2v_train(X_train, vectorizer):** \n",
    "* The function *transform_w2v_train* generates the Word2Vec representation of the training data X_train based on the vectorizer and returns the representation and the corresponding mean vector.\n",
    "\n",
    "**transform_w2v_test(X_test, vectorizer, mean_vec):** \n",
    "* The function *transform_w2v_test* returns the Word2Vec representation of the test data based on the vectorizers and the mean vector.\n",
    "\n",
    "**train_w2v(model_type, X_train, y_train, param, vectorizer):** \n",
    "* The function *train_w2v* trains a model based on the Word2Vec representation of the training data, i.e. w2v_train(X_train, vectorizer), and returns both the trained model and the mean vector.\n",
    "* The parameter *model_type* can be chosen from the set $\\{$'SVC', 'RidReg', 'Boost-w2v', 'RandFor-w2v'$\\}$.\n",
    "* The parameter *param*  represents the number of estimators ('RandFor-w2v' and 'Boost-w2v') or the regularization parameter C and alpha for 'SVC' and 'LogReg', respectively.\n",
    "\n",
    "**validate_w2v(model_type, X_train, X_test, y_train, y_test, param):** \n",
    "* The function *validate_w2v* trains the model train_w2v(model_type, X_train, y_train, param, vectorizer) and returns a confusion matrix based on the test data (X_test, y_test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def transform_w2v_train(X_train, vectorizer):\n",
    "    \n",
    "    with vectorizer.disable_pipes():\n",
    "        train_vectors = np.array([vectorizer(text).vector for text in X_train])\n",
    "        \n",
    "    mean_vec = train_vectors.mean(axis=0)\n",
    "    X_train = train_vectors - mean_vec\n",
    "    return X_train, mean_vec\n",
    "\n",
    "def transform_w2v_test(X_test, vectorizer, mean_vec):\n",
    "    \n",
    "    with vectorizer.disable_pipes():\n",
    "        test_vectors = np.array([vectorizer(text).vector for text in X_test])\n",
    "\n",
    "    X_test = test_vectors - mean_vec\n",
    "    return X_test\n",
    "\n",
    "def train_w2v(model_type, X_train, y_train, param, vectorizer):\n",
    "    X_train, mean_vec = transform_w2v_train(X_train, vectorizer)\n",
    "    \n",
    "    if model_type == 'SVC':\n",
    "        model = SVC(C=param, class_weight='balanced')\n",
    "    elif model_type == 'RidReg':\n",
    "        model = RidgeClassifier(alpha=param, class_weight='balanced')\n",
    "    elif model_type == 'Boost-w2v':\n",
    "        base_est = DecisionTreeClassifier(max_depth=3)\n",
    "        model = AdaBoostClassifier(base_estimator=base_est, n_estimators=param)\n",
    "    elif model_type == 'RandFor-w2v':\n",
    "        model = RandomForestClassifier(n_estimators=param, class_weight='balanced')\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    return model, mean_vec\n",
    "\n",
    "def validate_w2v(model_type, X_train, X_test, y_train, y_test, param):\n",
    "    \n",
    "    vectorizer = spacy.load('en_core_web_lg')\n",
    "    model, mean_vec = train_w2v(model_type, X_train, y_train, param, vectorizer)\n",
    "    X_test = transform_w2v_test(X_test, vectorizer, mean_vec)\n",
    "    predicted_labels = model.predict(X_test)\n",
    "    return confusion_matrix(y_test,predicted_labels, labels=[1,0,-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the cross-validation procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**cross_validation(representation, X_train, y_train, model, kf):** \n",
    "* The function *cross_validation* estimates the out-of-sample precision of the *model* based on the *representation*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(representation, X_train, y_train, model, kf):\n",
    "    precision = []\n",
    "    for train_idx, val_idx in kf.split(index_test):\n",
    "        \n",
    "        X_train_cv = X_train.iloc[train_idx]\n",
    "        X_val = X_train.iloc[val_idx]\n",
    "        y_train_cv = y_train.iloc[train_idx]\n",
    "        y_val = y_train.iloc[val_idx]\n",
    "        if representation == 'bow':\n",
    "            confusion_mat = validate_bow(X_train_cv, X_val, y_train_cv , y_val, model)\n",
    "            \n",
    "        elif representation == 'tfidf':\n",
    "            confusion_mat = validate_tfidf(model[0], X_train_cv, X_val, y_train_cv , y_val, model[1])\n",
    "            \n",
    "        elif representation == 'word2vector':\n",
    "            confusion_mat = validate_w2v(model[0], X_train_cv, X_val, y_train_cv , y_val, model[1])\n",
    "            \n",
    "        precision.append(np.trace(confusion_mat)/np.sum(confusion_mat)) \n",
    "    \n",
    "    return np.mean(precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose the model with the lowest estimated out-of-sample error based on the CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, the model with the lowest (estimated) out-of-sample error, based on a 10-fold cross-validation is determined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bow', 0.6321428571428572]\n",
      "['simple_cnn', 0.5892857142857142]\n",
      "['ensemble', 0.5964285714285714]\n",
      "['LogReg', 0.1, 0.6]\n",
      "['LogReg', 0.2, 0.5857142857142856]\n",
      "['LogReg', 0.3, 0.5821428571428571]\n",
      "['LogReg', 0.4, 0.6071428571428572]\n",
      "['LogReg', 0.5, 0.6249999999999999]\n",
      "['LogReg', 0.6, 0.6607142857142857]\n",
      "['LogReg', 0.7, 0.6392857142857143]\n",
      "['LogReg', 0.8, 0.65]\n",
      "['LogReg', 0.9, 0.6500000000000001]\n",
      "['LogReg', 1.0, 0.6464285714285714]\n",
      "['Boost-tfidf', 50, 0.5464285714285715]\n",
      "['Boost-tfidf', 100, 0.5535714285714286]\n",
      "['Boost-tfidf', 150, 0.5678571428571428]\n",
      "['Boost-tfidf', 200, 0.5678571428571428]\n",
      "['RandFor-tfidf', 50, 0.6142857142857143]\n",
      "['RandFor-tfidf', 100, 0.6249999999999999]\n",
      "['RandFor-tfidf', 150, 0.6428571428571429]\n",
      "['RandFor-tfidf', 200, 0.6285714285714287]\n",
      "['SVC', 1, 0.642857142857143]\n",
      "['SVC', 2, 0.642857142857143]\n",
      "['SVC', 3, 0.6785714285714286]\n",
      "['SVC', 4, 0.6964285714285714]\n",
      "['SVC', 5, 0.6785714285714286]\n",
      "['SVC', 6, 0.6821428571428572]\n",
      "['SVC', 7, 0.6892857142857143]\n",
      "['SVC', 8, 0.6964285714285714]\n",
      "['SVC', 9, 0.7]\n",
      "['SVC', 10, 0.6892857142857143]\n",
      "['RidReg', 0.1, 0.6107142857142858]\n",
      "['RidReg', 0.2, 0.6071428571428572]\n",
      "['RidReg', 0.3, 0.6464285714285715]\n",
      "['RidReg', 0.4, 0.6]\n",
      "['RidReg', 0.5, 0.6214285714285714]\n",
      "['RidReg', 0.6, 0.6107142857142858]\n",
      "['RidReg', 0.7, 0.5678571428571428]\n",
      "['RidReg', 0.8, 0.6178571428571428]\n",
      "['RidReg', 0.9, 0.6285714285714287]\n",
      "['RidReg', 1.0, 0.575]\n",
      "['Boost-w2v', 50, 0.5892857142857142]\n",
      "['Boost-w2v', 100, 0.6500000000000001]\n",
      "['Boost-w2v', 150, 0.6642857142857144]\n",
      "['Boost-w2v', 200, 0.6428571428571429]\n",
      "['RandFor-w2v', 50, 0.6392857142857145]\n",
      "['RandFor-w2v', 100, 0.6178571428571428]\n",
      "['RandFor-w2v', 150, 0.6392857142857142]\n",
      "['RandFor-w2v', 200, 0.6214285714285713]\n",
      "['SVC', 9]\n"
     ]
    }
   ],
   "source": [
    "n_folds = 10\n",
    "index_test = range(len(y_train))\n",
    "kf = KFold(n_splits=n_folds, shuffle=True)\n",
    "\n",
    "bow_models = [[architecture] for architecture in ['bow', 'simple_cnn', 'ensemble']]\n",
    "\n",
    "tfidf_models = [['LogReg', x/10] for x in range(1,11)] + \\\n",
    "    [['Boost-tfidf', x] for x in [50, 100, 150,200]] + \\\n",
    "    [['RandFor-tfidf', x] for x in [50, 100, 150,200]]\n",
    "    \n",
    "\n",
    "w2v_models = [['SVC', x] for x in range(1,11)] + \\\n",
    "    [['RidReg', x/10] for x in range(1,11)] + \\\n",
    "    [['Boost-w2v', x] for x in [50, 100, 150,200]] + \\\n",
    "    [['RandFor-w2v', x] for x in [50, 100, 150,200]]\n",
    "\n",
    "best_model = None\n",
    "best_precision = 0\n",
    "for representation in ['bow', 'tfidf', 'word2vector']:\n",
    "\n",
    "    if representation == 'bow':\n",
    "        model_list = bow_models\n",
    "    elif representation == 'tfidf':\n",
    "        model_list = tfidf_models\n",
    "    elif representation == 'word2vector':\n",
    "        model_list = w2v_models\n",
    "        \n",
    "    for model in model_list:\n",
    "        model_precision = cross_validation(representation, X_train, y_train, model, kf)\n",
    "        print(model + [model_precision])\n",
    "        if (best_model is None) | (model_precision > best_precision):\n",
    "            best_model = model\n",
    "            best_precision = model_precision\n",
    "                \n",
    "print(best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model with the lowest (estimated) out-of-sample error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, the model with the lowest (estimated) out-of-sample error is trained based on the whole data set. The expected out-of-sample error is (at least theoretically) less than or equal to the out-of-sample error of the model based on less data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if best_model in bow_models:\n",
    "    final_model = train_bow(X_train, y_train, best_model[0])\n",
    "    dump(final_model, 'final_model.joblib') \n",
    "elif best_model in tfidf_models:\n",
    "    X_train, vectorizer = transform_tfidf_train(X_train)\n",
    "    final_model = train_tfidf(best_model[0], X_train, y_train, best_model[1])\n",
    "    dump(final_model, 'final_model.joblib') \n",
    "    dump(vectorizer, 'vectorizer.joblib') \n",
    "elif best_model in w2v_models:\n",
    "    vectorizer = spacy.load('en_core_web_lg')\n",
    "    final_model, mean_vec = train_w2v(best_model[0], X_train, y_train, best_model[1], vectorizer)\n",
    "    dump(final_model, 'final_model.joblib') \n",
    "    dump(mean_vec, 'mean_vec.joblib')\n",
    "    #dump(vectorizer, 'vectorizer.joblib') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\"In sample\" confusion matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[175   0   0]\n",
      " [  0  70   0]\n",
      " [  0   0  35]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = final_model.predict(transform_w2v_test(X_train, vectorizer, mean_vec))\n",
    "print(confusion_matrix(y_train,y_pred, labels=[1,0,-1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
